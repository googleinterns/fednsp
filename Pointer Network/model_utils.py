"""This file defines the utils for the model traning.

This file contains functions to compute the loss
and metrics to bes used while training including
functions to compute the F1 score and
sentence-level semantic frame accuracy, etc.

Some of the code was in this file was taken from
https://github.com/ZephyrChenzf/SF-ID-Network-For-NLU/blob/master/utils.py
"""

import tensorflow as tf
import numpy as np

from data_utils import create_masks

SOS_TOKEN = '__SOS'

class loss_function(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        """Defines the loss function to be used while training the model.
        The loss is defined as the sum of the intent loss and the slot loss
        per token. Masking is used to not compute the loss on the padding tokens.

        Args:
        slot_real: The ground truth for the slots.
        intent_real: The ground through for the intents.
        slot_pred: The slots predicted by the model.
        intent_pred: The intents predicted by the model.
        intent_loss_objective: The objective used to compute the intent loss.
        slot_loss_objective: The objective used to compute the slot loss.

        Returns:
        The total loss.
        """

        loss_objective = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True, reduction='none')

        mask = tf.math.logical_not(tf.math.equal(y_true, 0))
        loss_ = loss_objective(y_true, y_pred)

        mask = tf.cast(mask, dtype=loss_.dtype)
        loss_ *= mask
        loss = tf.reduce_sum(loss_) / tf.reduce_sum(mask)

        return loss

class IntentSlotAccuracy(tf.keras.metrics.Metric):
    """Class defines the intent + slot accuracy metric to be used
    for the given task.
    """
    def __init__(self,
                 name='intent_slot_accuracy',
                 **kwargs):
        super(IntentSlotAccuracy, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.total = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true[0], 0)),
                            tf.int32)
        tp = tf.cast(
            tf.math.equal(
                y_true[0],
                tf.math.argmax(y_pred[0], axis=-1, output_type=tf.int32)),
            tf.int32)

        accuracy = tf.math.reduce_sum(tf.math.multiply(
            tp, mask),
                                           axis=-1)
        accuracy = tf.math.equal(accuracy,
                                      tf.math.reduce_sum(mask, axis=-1))

        tp = tf.reduce_sum(tf.cast(accuracy, tf.float32))

        total = tf.shape(y_true[0])[0]

        self.true_positives.assign_add(tp)
        self.total.assign_add(tf.cast(total, tf.float32))

    def result(self):
        return tf.math.divide(self.true_positives, self.total)

    def reset_states(self):
        self.true_positives.assign(0)
        self.total.assign(0)


def compute_acc(gt_outputs, pred_outputs):
    """Computes the semantic accuracy of the intent and slot predictions.
    (The percentage of queries for which both intents and slots were
    predicted correctly.)

    Args:
    slot_real: The ground truth for the slots.
    intent_real: The ground through for the intents.
    slot_pred: The slots predicted by the model.
    intent_pred: The intents predicted by the model.

    Returns:
    The sematic accuracy of the predictions.
    """

    mask = (gt_outputs != 0)
    
    matches = (pred_outputs == gt_outputs) * mask

    seq_lens = np.sum(mask, axis=-1)
    matches_sum = np.sum(matches, axis=-1)

    acc = np.mean(seq_lens==matches_sum) * 100.0

    return acc

def evaluate(model, dataset, out_vocab, max_len=66):
    """Evaluates the performance of the model on the given dataset and
    prints out the metrics.

    Args:
    model: The model to be evaluated.
    dataset: The dataset on which the model is to be evaluated.
    slot_vocab: The vocabulary of slot labels.
    max_len: The number of outputs to be generated by the decoder.
    """

    pred_outputs = []
    gt_outputs = []

    for input, gt_output in dataset:

        decoder_input = [out_vocab['vocab'][SOS_TOKEN]] * input.shape[0]
        output = tf.expand_dims(decoder_input, 1)

        for i in range(max_len - 1):
            padding_mask, look_ahead_mask, pointer_mask = create_masks(
                input, output)

            pred_output = model(input, output, padding_mask,
                                          look_ahead_mask, pointer_mask)

            # select the last word from the seq_len dimension
            pred_output = pred_output[:, -1:, :]  # (batch_size, 1, vocab_size)

            pred_output = tf.cast(tf.argmax(pred_output, axis=-1), tf.int32)

            output = tf.concat([output, pred_output], axis=-1)

        pred_outputs.append(output.numpy())
        gt_outputs.append(gt_output.numpy().squeeze())

    pred_outputs = np.vstack(pred_outputs)
    gt_outputs = np.vstack(gt_outputs)

    accuracy = compute_acc(gt_outputs, pred_outputs)

    print("Accuracy {:.4f}".format(
        accuracy))

    return accuracy

def write_predictions_to_file(model, dataset, out_path, input_path, in_vocab, out_vocab, max_len=66):

    pred_outputs = []
    inputs = []

    for input, _ in dataset:

        decoder_input = [out_vocab['vocab'][SOS_TOKEN]] * input.shape[0]
        output = tf.expand_dims(decoder_input, 1)

        for i in range(max_len - 1):
            padding_mask, look_ahead_mask, pointer_mask = create_masks(
                input, output)

            pred_output = model(input, output, padding_mask,
                                            look_ahead_mask, pointer_mask)

            # select the last word from the seq_len dimension
            pred_output = pred_output[:, -1:, :]  # (batch_size, 1, vocab_size)

            pred_output = tf.cast(tf.argmax(pred_output, axis=-1), tf.int32)

            output = tf.concat([output, pred_output], axis=-1)

        pred_outputs.append(output.numpy())

    pred_outputs = np.vstack(pred_outputs)

    decoded_preds = []
    input_fd = open(input_path, 'r')
    for inp in input_fd:
        i = [token.strip() for token in inp.split(' ')]
        inputs.append(i)

    for input, pred in zip(inputs, pred_outputs):
    

        output_decoded = []
        for output_token in pred:
            if output_token == 3:
                break
            decoded_token = out_vocab['rev'][output_token]
            if decoded_token.startswith('@ptr'):
                output_decoded.append(input[int(decoded_token.split('_')[-1])])
            else:
                if ']' in decoded_token:
                    decoded_token = ']'
                output_decoded.append(decoded_token)
        decoded_preds.append(' '.join(output_decoded[1:]))

         
    with open(out_path, 'w+') as out:
         for output in decoded_preds:
            out.write(output+'\n')
