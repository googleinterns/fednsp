"""This file defines the utils for the model traning.

This file contains functions to compute the loss
and metrics to be used to evaluate the model performance.
"""

import tensorflow as tf
import numpy as np

from data_utils import create_masks

SOS_TOKEN = '__SOS'


class MaskedLoss(tf.keras.losses.Loss):
    """Defines the loss function to be used while training the model.
    Masking is used to not compute the loss on the padding tokens.
    """
    def call(self, y_true, y_pred):
        """
        Args:
        y_true: The ground truth for the annotations.
        y_pred: The annotations predicted by the model.

        Returns:
        The total loss per output token.
        """

        loss_objective = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True, reduction='none')

        mask = tf.math.logical_not(tf.math.equal(y_true, 0))
        loss_ = loss_objective(y_true, y_pred)

        mask = tf.cast(mask, dtype=loss_.dtype)
        loss_ *= mask
        loss = tf.reduce_sum(loss_) / tf.reduce_sum(mask)

        return loss


class IntentSlotAccuracy(tf.keras.metrics.Metric):
    """Class defines the intent + slot accuracy metric to be used
    for the given task.
    """
    def __init__(self, name='intent_slot_accuracy', **kwargs):
        super(IntentSlotAccuracy, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.total = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true[0], 0)),
                       tf.int32)
        tp = tf.cast(
            tf.math.equal(
                y_true[0],
                tf.math.argmax(y_pred[0], axis=-1, output_type=tf.int32)),
            tf.int32)

        accuracy = tf.math.reduce_sum(tf.math.multiply(tp, mask), axis=-1)
        accuracy = tf.math.equal(accuracy, tf.math.reduce_sum(mask, axis=-1))

        tp = tf.reduce_sum(tf.cast(accuracy, tf.float32))

        total = tf.shape(y_true[0])[0]

        self.true_positives.assign_add(tp)
        self.total.assign_add(tf.cast(total, tf.float32))

    def result(self):
        return tf.math.divide(self.true_positives, self.total)

    def reset_states(self):
        self.true_positives.assign(0)
        self.total.assign(0)


def compute_acc(gt_outputs, pred_outputs):
    """Computes the accuracy of the intent and slot predictions.
    (The percentage of queries for which both intents and slots were
    predicted correctly.)

    Args:
    gt_outputs: The ground truth for the annotations.
    pred_outputs: The annotations predicted by the model.

    Returns:
    The sentence-wise accuracy of the predictions.
    """

    mask = (gt_outputs != 0)

    matches = (pred_outputs == gt_outputs) * mask

    seq_lens = np.sum(mask, axis=-1)
    matches_sum = np.sum(matches, axis=-1)

    acc = np.mean(seq_lens == matches_sum) * 100.0

    return acc


def evaluate(model, dataset, out_vocab, max_len=66):
    """Evaluates the performance of the model on the given dataset and
    prints out the metrics.

    Args:
    model: The model to be evaluated.
    dataset: The dataset on which the model is to be evaluated.
    out_vocab: The vocabulary of output labels.
    max_len: The number of outputs to be generated by the decoder.
    """

    pred_outputs = []
    gt_outputs = []

    for input_batch, gt_output in dataset:

        decoder_input = [out_vocab['vocab'][SOS_TOKEN]] * input_batch.shape[0]
        output = tf.expand_dims(decoder_input, 1)

        for _ in range(max_len - 1):
            padding_mask, look_ahead_mask, pointer_mask = create_masks(
                input_batch, output)

            pred_output = model(
                (input_batch, output, padding_mask, look_ahead_mask, pointer_mask),
                training=False)

            # select the last word from the seq_len dimension
            pred_output = pred_output[:, -1:, :]  # (batch_size, 1, vocab_size)

            pred_output = tf.cast(tf.argmax(pred_output, axis=-1), tf.int32)

            output = tf.concat([output, pred_output], axis=-1)

        pred_outputs.append(output.numpy())
        gt_outputs.append(gt_output.numpy().squeeze())

    pred_outputs = np.vstack(pred_outputs)
    gt_outputs = np.vstack(gt_outputs)

    accuracy = compute_acc(gt_outputs, pred_outputs)

    print("Accuracy {:.4f}".format(accuracy))

    return accuracy


def write_predictions_to_file(model,
                              dataset,
                              out_path,
                              input_path,
                              out_vocab,
                              max_len=66):
    """Decodes the outputs of the decoder of the model and writes the predictions to a file

    Args:
    model: The model to be evaluated.
    dataset: The dataset on which the model is to be evaluated.
    out_path: The path of the file to with the predictions have to be saved.
    input_path: The path to the file containing the input queries.
    out_vocab: The vocabulary of output labels.
    max_len: The number of outputs to be generated by the decoder.
    """
    pred_outputs = []
    inputs = []

    # Generate the predictions
    for input_batch, _ in dataset:

        decoder_input = [out_vocab['vocab'][SOS_TOKEN]] * input_batch.shape[0]
        output = tf.expand_dims(decoder_input, 1)

        for i in range(max_len - 1):
            padding_mask, look_ahead_mask, pointer_mask = create_masks(
                input_batch, output)

            pred_output = model(input_batch, output, padding_mask, look_ahead_mask,
                                pointer_mask)

            # select the last word from the seq_len dimension
            pred_output = pred_output[:, -1:, :]  # (batch_size, 1, vocab_size)

            pred_output = tf.cast(tf.argmax(pred_output, axis=-1), tf.int32)

            output = tf.concat([output, pred_output], axis=-1)

        pred_outputs.append(output.numpy())

    pred_outputs = np.vstack(pred_outputs)

    decoded_preds = []

    # Generate the input tokens to be used to decode the pointers
    input_fd = open(input_path, 'r')
    for inp in input_fd:
        i = [token.strip() for token in inp.split(' ')]
        inputs.append(i)

    # Decode the precitions
    for input_batch, pred in zip(inputs, pred_outputs):
        output_decoded = []
        for output_token in pred:
            if output_token == 3:
                break
            decoded_token = out_vocab['rev'][output_token]
            if decoded_token.startswith('@ptr'):
                output_decoded.append(input_batch[int(decoded_token.split('_')[-1])])
            else:
                if ']' in decoded_token:
                    decoded_token = ']'
                output_decoded.append(decoded_token)
        decoded_preds.append(' '.join(output_decoded[1:]))

    # Write decoded predictions to file
    with open(out_path, 'w+') as out:
        for output in decoded_preds:
            out.write(output + '\n')
